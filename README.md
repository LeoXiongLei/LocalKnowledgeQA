# LocalKnowledgeQA
本地文本问答系统
# 本地文本问答系统

本地文本问答系统设计文档
I. 引言
A. 文档目的
本文档的目的是为我们的本地文档问答系统的设计和开发过程提供全面的概述。它作为产品团队和开发人员参与项目的指南，概述了系统的目标、范围和需求。本文档将确保对项目目标有共同的理解，并在开发过程中充当参考资料。
B. 项目范围
本项目的范围是基于LLM设计和开发一个本地文档问答系统。该系统将利用语言建模和自然语言处理的能力，针对用户上传的文档，为用户的查询提供答案。它将涉及导入文档、文档embedding化、检索、问答等功能。
该项目将专注于以下关键方面：
1. 导入文档：用户可以上传本地文档，利用LLM能力，解决自己的个性化问答需求。
2. 文档embedding化：通过调用相关函数，将文档embedding化。
3. 检索和问答：实现语义查询以及基于上传的文档进行问答。
C. 目标受众
本文档的目标受众包括：
1. 产品团队：负责设计和开发智能知识库问答系统。
2. 开发人员：参与数据训练与系统搭建。
II. 背景和目标
A. 背景
大模型拥有强大的能力，可以说ChatGPT已掌握人类目前的大多数知识，但是，其也存在相当大的限制，无法及时更新认知，并且对于很多未经投喂或者投喂资料较少的领域回答效果并不理想。embedding可以很好的弥补LLM这方面的缺陷，基于这点，给大模型装上“海马体”，让大模型可以针对个人特定资料进行回答是一个很有吸引力的想法，可以提高我们在某些方面的工作效率，因此我们对这个想法很感兴趣，在经过学习和实践后，复现了这个项目。
C. 目标
我们的项目目标如下：
1. 建立文档上传功能：允许用户上传个人本地文档。
2. 基于上传资料进行问答：让ChatGPT能够基于我们自己的文档进行回答，提高效率。
3. 给ChatGPT装上“海马体”：让ChatGPT能够回答一些之前不能回答的问题，帮助我们进行数据、知识等的分析、检索、学习等活动。
III. 实现方式
简述我们的实现方式。
1. 调用Pinecone API将上传的文本embedding化
2. 调用langchain相关函数，实现检索问答功能
IV.用户故事
A. 目标用户
1. 高校教师、学生等：他们需要阅读大量论文，通过本地的知识文档问答系统，可以快速阅读论文，提高信息获取效率。
2. 小型公司：通过上传公司的资料数据等、建立私人资料库，可以帮助小型公司更好地使用大模型。
3. 数据分析师：通过直接上传数据文件，可以直接让大模型帮助进行初步数据分析，可以大大提高数据分析师的工作效率。
B. 用户故事
1. 作为用户，我希望能提交咨询问题并根据嵌入的法律知识库获得准确的答案。
2. 作为用户，我希望系统理解我的上下文并提供相关的回答。
3. 作为用户，我希望有选择的定制知识库，可以添加或删除特定领域。
4. 作为用户，我希望系统记住我的偏好，并根据我的先前互动提供个性化的回答。
V.验收标准
1. 用户故事：查询和回答
   - 用户收到符合法律条文的、适应自身实际情境的答案。
   - 回答及时生成，确保流畅的用户体验。
2. 用户故事：上下文理解
   - 系统能够理解用户查询的上下文，从而提供更准确的回答。
   - 上下文信息在生成回答时有效利用。
3. 用户故事：知识库定制
   - 用户可以轻松通过添加或删除特定领域或主题来定制知识库。
   - 定制更改准确反映在系统的回答中。
4. 用户故事：反馈循环
   -用户可以对系统的回答提供反馈，并将反馈积极纳入系统改进。
   - 系统根据用户反馈在回答质量上呈现迭代改进。
通过满足这些验收标准，我们可以确保本地文档问答系统实现其目标，为用户提供准确和个性化的答案。
VII. 风险与缓解策略
识别潜在风险并制定缓解策略对于确保本地文档问答系统项目的成功完成至关重要。通过积极应对风险，项目团队可以将其对项目时间表和交付成果的影响降到最低。以下是一些常见的风险及相应的缓解策略：
1. 用户接受和采用风险：
   - 风险：用户可能不完全接受或使用智能知识库问答系统。
   - 缓解：进行用户调研和可用性测试，了解用户偏好和需求。融入用户反馈，并在系统的设计和功能上进行迭代。尝试让模型拥有自检能力，可以通过联网或者其他大模型监督等方式对回答进行检查，提高回答置信度。
2. 安全和隐私风险：
   - 风险：在处理用户数据和系统交互时可能存在潜在的漏洞和隐私问题。
   - 缓解：对于安全要求较高的朋友可以选择本地部署模型如ChatGLM。
VIII. 总结
在本次项目中，我们通过学习和实践，实现了本地文档问答系统，增强了动手能力，对AI及生成式大模型的原理和开发范式有了更进一步的了解，非常感谢社区提供此次机会，接下来我们希望可以继续交流，探索大模型更深层次的应用。


# 实现思路
1.把本地数据集（无论是抓取还是接口）先用OpenAi的对每个文章做个摘要，并存储到本地
2.对步骤1的摘要调用Embedding接口去向量化后，存储到我本地
以上步骤完成了文章的摘要及摘要的向量化
3.自助文档系统有输入接口，输出接口
4.当用户输入问题后，就不通过OPENAI接口了，而是本地使用机器学习模型，比如5T BASE, ChatGLM-6B 这些消费级显卡就能基本跑起来的模型，
去对输入问题和本地库里之前被OPENAI向量化后的摘要数据，进行相似度计算（比如余弦相似度），对排名靠前的三个文章提取出来。
5.通过本地的模型（5T BASE, ChatGLM-6B ）去按自然语言，把提取的文章 进行回答给用户，作为输出



贡献者：
设计文档：贺新凉
实现思路：Leo




